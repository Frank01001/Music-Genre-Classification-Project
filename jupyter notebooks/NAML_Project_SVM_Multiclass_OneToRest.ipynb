{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NAML_Project_SVM_Multiclass_OneToRest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multiclass SVM: One To Rest"
      ],
      "metadata": {
        "id": "-a7Wla1M6dq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the notebook used to train the multiclass classifier that uses One To Rest SVM."
      ],
      "metadata": {
        "id": "0GdYd2Pg7VlC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rDm0aLYZZ_S2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genre_names = ['blues', 'classical', 'country', 'disco', 'hipop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "\n",
        "# Dataset import\n",
        "data = pd.read_csv('extracted_dataset.csv')\n",
        "\n",
        "# Dataset normalization\n",
        "data_mean = data.mean()\n",
        "data_std = data.std()\n",
        "\n",
        "data_normalized = (data - data_mean) / data_std\n",
        "\n",
        "dataset = data_normalized.to_numpy()[:, 1:4]\n",
        "labels = data.to_numpy()[:, 4]"
      ],
      "metadata": {
        "id": "qXCepUQFd6Sx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separation of the training set and the validation set"
      ],
      "metadata": {
        "id": "hFOuOTBabtSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Indexes extraction\n",
        "indexes_train = np.random.choice(1000, 800, replace=False)\n",
        "indexes_valid= np.setdiff1d(np.array([i for i in range(1000)]), indexes_train)\n",
        "np.random.shuffle(indexes_valid)\n",
        "\n",
        "dataset_train = dataset[indexes_train, :]\n",
        "dataset_valid = dataset[indexes_valid, :]\n",
        "\n",
        "labels_train = labels[indexes_train]\n",
        "labels_valid = labels[indexes_valid]"
      ],
      "metadata": {
        "id": "hgXCU4hOb2nW"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dual with kernel trick"
      ],
      "metadata": {
        "id": "-sn6U6t6e-H6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The methodologies used is the resolution of the dual problem of the SVM algorithm with the usage of the kernel trick."
      ],
      "metadata": {
        "id": "7IzJ7b727tfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import scipy.optimize as opt"
      ],
      "metadata": {
        "id": "kS_oBj7XfDZf"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The hessian is always equal to zero because the constraint is linear\n",
        "def hessian(x):\n",
        "  return np.zeros((800, 800))\n",
        "\n",
        "#This is the kernel function for the matrices\n",
        "def kernel_mat(xi, xj):\n",
        "  return (jnp.dot(xi, xj) + np.ones((N, N)))**3\n",
        "\n",
        "#This is the kernel function for the vectors\n",
        "def kernel_vec(xi, xj):\n",
        "  return (np.dot(xi, xj) + 1)**3\n",
        "\n",
        "#This is the objective function of the dual problem with the usage of the kernel trick\n",
        "def obj_kernel(c):\n",
        "  partial_1 = -jnp.sum(c)\n",
        "\n",
        "  c_outer = jnp.outer(c, c)\n",
        "  c_outer = jnp.triu(c_outer)\n",
        "\n",
        "  y_outer = jnp.outer(in_labels, in_labels)\n",
        "  y_outer = jnp.triu(y_outer)\n",
        "\n",
        "  K= kernel_mat(in_data, in_data.T)\n",
        "\n",
        "  partial_2= 0.5 * jnp.sum(c_outer * y_outer * K)\n",
        "\n",
        "  return partial_1 + partial_2"
      ],
      "metadata": {
        "id": "if_AotWvf9Fi"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N= dataset_train.shape[0]\n",
        "\n",
        "#The weight used for the method\n",
        "w_phi = np.zeros(N)\n",
        "\n",
        "#The biases computed and saved in the .csv\n",
        "B = np.zeros(len(genre_names))\n",
        "\n",
        "#The results of the dual problem computed and saved in the .csv\n",
        "C= np.zeros((len(genre_names), N))\n",
        "\n",
        "#Iteration on the genres\n",
        "for i in range(10):\n",
        "  #Data of the genre considered\n",
        "  class_genre = dataset_train[labels_train==i, :]\n",
        "\n",
        "  #Data of the other genres\n",
        "  class_others = dataset_train[labels_train!=i, :]\n",
        "\n",
        "  #Data and labels used in the resolution of the dual problem\n",
        "  in_data = np.concatenate((class_genre, class_others), axis=0)\n",
        "  in_labels = np.concatenate((np.ones(class_genre.shape[0]), -1 * np.ones(class_others.shape[0])), axis=None)\n",
        "\n",
        "  a = np.zeros(N)\n",
        "  obj_k_jit=jax.jit(obj_kernel)\n",
        "\n",
        "  linear_constraint = opt.LinearConstraint(in_labels, 0, 0, keep_feasible=True)\n",
        "  res = opt.minimize(obj_k_jit, a, method='trust-constr', jac='2-point', hess=hessian, constraints=[linear_constraint], options={'maxiter': 1000}, bounds=opt.Bounds(np.zeros(N), np.ones(N)*np.inf))\n",
        "\n",
        "  C[i, :] = np.array(res.x)\n",
        "\n",
        "  index_non_zero = -1\n",
        "  for j in range(N):\n",
        "    w_phi[j] = np.sum(np.array([C[i, k]*in_labels[k]*kernel_vec(dataset_train[j, :], dataset_train[k, :]) for k in range(N)]))\n",
        "    if C[i, j]>0 and index_non_zero<0:\n",
        "      index_non_zero = j\n",
        "\n",
        "  B[i] = - in_labels[index_non_zero] + w_phi[index_non_zero]\n",
        "\n",
        "  confusion_mat = np.zeros((2,2))\n",
        "\n",
        "  print('%s done, start testing' % genre_names[i])\n",
        "\n",
        "  #Computation of the confusion matrix for one genre\n",
        "  for test_index in range(class_genre.shape[0]):\n",
        "    predicted = 0 if np.sign(w_phi[test_index] - B[i]) >= 0 else 1\n",
        "    confusion_mat[0,predicted] += 1\n",
        "\n",
        "  for test_index in range(class_genre.shape[0], class_genre.shape[0] + class_others.shape[0]):\n",
        "    predicted = 0 if np.sign(w_phi[test_index] - B[i]) >= 0 else 1\n",
        "    confusion_mat[1,predicted] += 1\n",
        "    \n",
        "  print(confusion_mat)\n",
        "  print(confusion_mat.trace() / confusion_mat.sum())"
      ],
      "metadata": {
        "id": "HrviUyargHYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "902eb2c8-db27-4c84-be54-f3067842ba01"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blues done, start testing\n",
            "[[ 28.  54.]\n",
            " [176. 542.]]\n",
            "0.7125\n",
            "classical done, start testing\n",
            "[[ 32.  52.]\n",
            " [204. 512.]]\n",
            "0.68\n",
            "country done, start testing\n",
            "[[ 35.  44.]\n",
            " [232. 489.]]\n",
            "0.655\n",
            "disco done, start testing\n",
            "[[ 28.  50.]\n",
            " [185. 537.]]\n",
            "0.70625\n",
            "hipop done, start testing\n",
            "[[ 32.  52.]\n",
            " [204. 512.]]\n",
            "0.68\n",
            "jazz done, start testing\n",
            "[[ 32.  52.]\n",
            " [204. 512.]]\n",
            "0.68\n",
            "metal done, start testing\n",
            "[[ 34.  49.]\n",
            " [209. 508.]]\n",
            "0.6775\n",
            "pop done, start testing\n",
            "[[ 28.  50.]\n",
            " [185. 537.]]\n",
            "0.70625\n",
            "reggae done, start testing\n",
            "[[ 22.  51.]\n",
            " [145. 582.]]\n",
            "0.755\n",
            "rock done, start testing\n",
            "[[ 21.  54.]\n",
            " [126. 599.]]\n",
            "0.775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we save the results of the training to use them in the classifier"
      ],
      "metadata": {
        "id": "Ri21Ru5X8dym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_C = pd.DataFrame(C)\n",
        "df_C.to_csv('C_kernel.csv')\n",
        "\n",
        "df_B = pd.DataFrame(B)\n",
        "df_B.to_csv('B_kernel.csv')\n",
        "\n",
        "df_indexes = pd.DataFrame(indexes_train)\n",
        "df_indexes.to_csv('indexes_kernel.csv')"
      ],
      "metadata": {
        "id": "BLtpaohRrE-d"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part implements the classifier one to rest with all the genres passed through the method paramenters. \n",
        "At the end, the one with higher \"score\" will be considered the most probably correct."
      ],
      "metadata": {
        "id": "XXQi6wGE8l1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier_oneToRest(input, genres=genre_names):\n",
        "  classifications = -1000 * np.ones(len(genre_names))\n",
        "\n",
        "  for genre in genres:\n",
        "    genre_index = genre_names.index(genre)\n",
        "\n",
        "    in_labels = np.zeros(N)\n",
        "    in_labels[labels_train==genre_index]=1\n",
        "    in_labels[labels_train!=genre_index]=-1\n",
        "\n",
        "    w_phi = np.sum(np.array([C[genre_index, i]*in_labels[i]*kernel_vec(input, dataset_train[i, :]) for i in range(N)]))\n",
        "    b = B[genre_index]\n",
        "\n",
        "    classifications[genre_index] = w_phi - b\n",
        "\n",
        "  return genre_names[np.argmax(classifications)]"
      ],
      "metadata": {
        "id": "NlsHhyYGrqL3"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix and accuracy for 10 genres"
      ],
      "metadata": {
        "id": "yAuIPkjs9AS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix = np.zeros((len(genre_names), len(genre_names)))\n",
        "\n",
        "for i in range(dataset_valid.shape[0]):\n",
        "  predicted = genre_names.index(classifier_oneToRest(dataset_valid[i, :]))\n",
        "  confusion_matrix [labels_valid[i].astype(int), predicted] += 1\n",
        "\n",
        "print(confusion_matrix)\n",
        "print(confusion_matrix.trace()/confusion_matrix.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tcSHfSxu7Wf",
        "outputId": "9894f19c-dfde-456c-b7a0-5bf6d0a85b6e"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  3.  9.  0.  0.  0.  4.  2.  0.  0.]\n",
            " [ 0.  9.  6.  0.  0.  0.  1.  0.  0.  0.]\n",
            " [ 0.  2. 19.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  4.  7.  0.  0.  0.  5.  4.  2.  0.]\n",
            " [ 0.  1.  9.  0.  4.  0.  1.  1.  0.  0.]\n",
            " [ 0.  5. 10.  0.  0.  0.  1.  0.  0.  0.]\n",
            " [ 0.  0.  3.  0.  0.  0. 12.  2.  0.  0.]\n",
            " [ 0.  2.  1.  0.  1.  0.  1. 17.  0.  0.]\n",
            " [ 0.  8. 15.  0.  3.  0.  1.  0.  0.  0.]\n",
            " [ 0.  1. 13.  0.  1.  0. 10.  0.  0.  0.]]\n",
            "0.305\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix and accuracy for 4 genres"
      ],
      "metadata": {
        "id": "9N-pUErE9FE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix = np.zeros((len(genre_names), len(genre_names)))\n",
        "\n",
        "dataset_reduced = dataset_valid[np.logical_or(np.logical_or(labels_valid==genre_names.index('blues'), labels_valid==genre_names.index('country')), np.logical_or(labels_valid==genre_names.index('disco'), labels_valid==genre_names.index('metal'))), :]\n",
        "labels_reduced = labels_valid[np.logical_or(np.logical_or(labels_valid==genre_names.index('blues'), labels_valid==genre_names.index('country')), np.logical_or(labels_valid==genre_names.index('disco'), labels_valid==genre_names.index('metal')))]\n",
        "\n",
        "for i in range(dataset_reduced.shape[0]):\n",
        "  predicted = genre_names.index(classifier_oneToRest(dataset_reduced[i, :], ['blues',  'country', 'disco', 'metal']))\n",
        "  confusion_matrix [labels_reduced[i].astype(int), predicted] += 1\n",
        "\n",
        "print(confusion_matrix)\n",
        "print(confusion_matrix.trace()/confusion_matrix.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy2FwbaZzt4Z",
        "outputId": "58aef2aa-1ca9-46e2-ba11-6c4d195c291d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0. 14.  0.  0.  0.  4.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0. 21.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0. 16.  0.  0.  0.  6.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  4.  0.  0.  0. 13.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
            "0.4358974358974359\n"
          ]
        }
      ]
    }
  ]
}