{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NAML_Project_SVM_Multiclass_OneToRest.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Multiclass SVM: One To Rest"
      ],
      "metadata": {
        "id": "-a7Wla1M6dq0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the notebook used to train the multiclass classifier that uses One To Rest SVM."
      ],
      "metadata": {
        "id": "0GdYd2Pg7VlC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "rDm0aLYZZ_S2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "genre_names = ['blues', 'classical', 'country', 'disco', 'hipop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
        "\n",
        "# Dataset import\n",
        "data = pd.read_csv('extracted_dataset.csv')\n",
        "\n",
        "# Dataset normalization\n",
        "data_mean = data.mean()\n",
        "data_std = data.std()\n",
        "\n",
        "data_normalized = (data - data_mean) / data_std\n",
        "\n",
        "dataset = data_normalized.to_numpy()[:, 1:4]\n",
        "labels = data.to_numpy()[:, 4]"
      ],
      "metadata": {
        "id": "qXCepUQFd6Sx"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Separation of the training set and the validation set"
      ],
      "metadata": {
        "id": "hFOuOTBabtSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Indexes extraction\n",
        "indexes_train = np.random.choice(1000, 800, replace=False)\n",
        "indexes_valid= np.setdiff1d(np.array([i for i in range(1000)]), indexes_train)\n",
        "np.random.shuffle(indexes_valid)\n",
        "\n",
        "dataset_train = dataset[indexes_train, :]\n",
        "dataset_valid = dataset[indexes_valid, :]\n",
        "\n",
        "labels_train = labels[indexes_train]\n",
        "labels_valid = labels[indexes_valid]"
      ],
      "metadata": {
        "id": "hgXCU4hOb2nW"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dual with kernel trick"
      ],
      "metadata": {
        "id": "-sn6U6t6e-H6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The methodologies used is the resolution of the dual problem of the SVM algorithm with the usage of the kernel trick."
      ],
      "metadata": {
        "id": "7IzJ7b727tfd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import scipy.optimize as opt"
      ],
      "metadata": {
        "id": "kS_oBj7XfDZf"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The hessian is always equal to zero because the constraint is linear\n",
        "def hessian(x):\n",
        "  return np.zeros((800, 800))\n",
        "\n",
        "#This is the kernel function for the matrices\n",
        "def kernel_mat(xi, xj):\n",
        "  return (jnp.dot(xi, xj) + np.ones((N, N)))**3\n",
        "\n",
        "#This is the kernel function for the vectors\n",
        "def kernel_vec(xi, xj):\n",
        "  return (np.dot(xi, xj) + 1)**3\n",
        "\n",
        "#This is the objective function of the dual problem with the usage of the kernel trick\n",
        "def obj_kernel(c):\n",
        "  partial_1 = -jnp.sum(c)\n",
        "\n",
        "  c_outer = jnp.outer(c, c)\n",
        "  c_outer = jnp.triu(c_outer)\n",
        "\n",
        "  y_outer = jnp.outer(in_labels, in_labels)\n",
        "  y_outer = jnp.triu(y_outer)\n",
        "\n",
        "  K= kernel_mat(in_data, in_data.T)\n",
        "\n",
        "  partial_2= 0.5 * jnp.sum(c_outer * y_outer * K)\n",
        "\n",
        "  return partial_1 + partial_2"
      ],
      "metadata": {
        "id": "if_AotWvf9Fi"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training with 10 genres"
      ],
      "metadata": {
        "id": "uw8WvuUxN76H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "N= dataset_train.shape[0]\n",
        "\n",
        "B = np.zeros(len(genre_names))\n",
        "\n",
        "#The results of the dual problem computed\n",
        "C = np.zeros((len(genre_names), N))\n",
        "\n",
        "#Iteration on the genres\n",
        "for i in range(10):\n",
        "  #Data of the genre considered\n",
        "  class_genre = dataset_train[labels_train==i, :]\n",
        "\n",
        "  #Data of the other genres\n",
        "  class_others = dataset_train[labels_train!=i, :]\n",
        "\n",
        "  #Data and labels used in the resolution of the dual problem\n",
        "  in_data = np.concatenate((class_genre, class_others), axis=0)\n",
        "  in_labels = np.concatenate((np.ones(class_genre.shape[0]), -1 * np.ones(class_others.shape[0])), axis=None)\n",
        "\n",
        "  a = np.zeros(N)\n",
        "  obj_k_jit=jax.jit(obj_kernel)\n",
        "\n",
        "  linear_constraint = opt.LinearConstraint(in_labels, 0, 0, keep_feasible=True)\n",
        "  res = opt.minimize(obj_k_jit, a, method='trust-constr', jac='2-point', hess=hessian, constraints=[linear_constraint], options={'maxiter': 1000}, bounds=opt.Bounds(np.zeros(N), np.ones(N)*np.inf))\n",
        "\n",
        "  C[i, :] = np.array(res.x)\n",
        "\n",
        "  index_non_zero = -1\n",
        "  for j in range(N):\n",
        "    if C[i, j] > 0 and index_non_zero < 0:\n",
        "      index_non_zero = j\n",
        "      break\n",
        "\n",
        "  w_phi = np.sum(np.array([C[i, j]*in_labels[j]*kernel_vec(dataset_train[index_non_zero, :], dataset_train[j, :]) for j in range(N)]))\n",
        "\n",
        "  B[i] = - in_labels[index_non_zero] + w_phi\n",
        "\n",
        "  print('%s done' % genre_names[i])"
      ],
      "metadata": {
        "id": "HrviUyargHYX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d10328-0604-4f9f-bd21-0f14d7cb69ec"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blues done\n",
            "classical done\n",
            "country done\n",
            "disco done\n",
            "hipop done\n",
            "jazz done\n",
            "metal done\n",
            "pop done\n",
            "reggae done\n",
            "rock done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part implements the classifier one to rest with all the genres passed through the method paramenters. \n",
        "At the end, the one with higher \"score\" will be considered the most probably correct."
      ],
      "metadata": {
        "id": "XXQi6wGE8l1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier_oneToRest(input):\n",
        "  classifications = -1000 * np.ones(len(genre_names))\n",
        "\n",
        "  for genre in genre_names:\n",
        "    genre_index = genre_names.index(genre)\n",
        "\n",
        "    in_labels = np.zeros(N)\n",
        "    in_labels[labels_train==genre_index]=1\n",
        "    in_labels[labels_train!=genre_index]=-1\n",
        "\n",
        "    w_phi = np.sum(np.array([C[genre_index, i]*in_labels[i]*kernel_vec(input, dataset_train[i, :]) for i in range(N)]))\n",
        "\n",
        "    classifications[genre_index] = w_phi - B[genre_index]\n",
        "\n",
        "  return genre_names[np.argmax(classifications)]"
      ],
      "metadata": {
        "id": "NlsHhyYGrqL3"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix and accuracy for 10 genres"
      ],
      "metadata": {
        "id": "yAuIPkjs9AS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix = np.zeros((len(genre_names), len(genre_names)))\n",
        "\n",
        "for i in range(dataset_valid.shape[0]):\n",
        "  predicted = genre_names.index(classifier_oneToRest(dataset_valid[i, :]))\n",
        "  confusion_matrix [labels_valid[i].astype(int), predicted] += 1\n",
        "\n",
        "print(confusion_matrix)\n",
        "print('Accuracy %1.2f %%' % (confusion_matrix.trace()/confusion_matrix.sum() * 100.0))"
      ],
      "metadata": {
        "id": "4tcSHfSxu7Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbd5d8b-4a36-49b2-ebab-a89646b92acc"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 5.  3.  0.  1.  1.  0.  5.  1.  0.  4.]\n",
            " [ 0. 15.  0.  1.  0.  5.  3.  0.  0.  1.]\n",
            " [ 3.  4.  2.  1.  0.  4.  6.  0.  0.  3.]\n",
            " [ 0.  3.  0.  3.  0.  0.  9.  2.  0.  0.]\n",
            " [ 0.  3.  0.  0.  0.  0.  8. 12.  0.  0.]\n",
            " [ 0. 11.  1.  0.  0.  4.  4.  0.  0.  1.]\n",
            " [ 0.  0.  1.  0.  0.  0. 12.  0.  0.  0.]\n",
            " [ 1.  0.  0.  0.  1.  1.  3. 13.  0.  0.]\n",
            " [ 0.  5.  0.  6.  2.  1.  5.  2.  0.  1.]\n",
            " [ 0.  4.  1.  1.  0.  0. 10.  0.  0.  1.]]\n",
            "Accuracy 27.50 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6 genres"
      ],
      "metadata": {
        "id": "dJgLHq3cSxLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genres = ['blues', 'classical', 'country', 'disco', 'pop', 'rock']\n",
        "\n",
        "# Indices of genres to classify\n",
        "genre_indices = list()\n",
        "for genre_n in genres: genre_indices.append(genre_names.index(genre_n))\n",
        "\n",
        "# Filter to only select samples from the input genres\n",
        "training_set_filter = np.zeros(dataset_train.shape[0], dtype='bool')\n",
        "for j in range(dataset_train.shape[0]):\n",
        "  training_set_filter[j] = labels_train[j] in genre_indices\n",
        "\n",
        "# Application of filter on the test set\n",
        "partial_training_set = dataset_train[training_set_filter,:]\n",
        "partial_training_labels = labels_train[training_set_filter]\n",
        "\n",
        "N= partial_training_set.shape[0]"
      ],
      "metadata": {
        "id": "OUDupMilTHou"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The hessian is always equal to zero because the constraint is linear\n",
        "def hessian(x):\n",
        "  return np.zeros((partial_training_set.shape[0], partial_training_set.shape[0]))"
      ],
      "metadata": {
        "id": "crqPM9v1YDlU"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The results of the dual problem computed\n",
        "C= np.zeros((len(genres), N))\n",
        "\n",
        "B = np.zeros(len(genres))\n",
        "\n",
        "#Iteration on the genres\n",
        "for i in range(len(genres)):\n",
        "  #Data of the genre considered\n",
        "  class_genre = partial_training_set[partial_training_labels==i, :]\n",
        "\n",
        "  #Data of the other genres\n",
        "  class_others = partial_training_set[partial_training_labels!=i, :]\n",
        "\n",
        "  #Data and labels used in the resolution of the dual problem\n",
        "  in_data = np.concatenate((class_genre, class_others), axis=0)\n",
        "  in_labels = np.concatenate((np.ones(class_genre.shape[0]), -1 * np.ones(class_others.shape[0])), axis=None)\n",
        "\n",
        "  a = np.zeros(N)\n",
        "  obj_k_jit=jax.jit(obj_kernel)\n",
        "\n",
        "  linear_constraint = opt.LinearConstraint(in_labels, 0, 0, keep_feasible=True)\n",
        "  res = opt.minimize(obj_k_jit, a, method='trust-constr', jac='2-point', hess=hessian, constraints=[linear_constraint], options={'maxiter': 1000}, bounds=opt.Bounds(np.zeros(N), np.ones(N)*np.inf))\n",
        "\n",
        "  C[i, :] = np.array(res.x)\n",
        "\n",
        "  index_non_zero = -1\n",
        "  for j in range(N):\n",
        "    if C[i, j] > 0 and index_non_zero < 0:\n",
        "      index_non_zero = j\n",
        "      break\n",
        "\n",
        "  w_phi = np.sum(np.array([C[i, j]*in_labels[j]*kernel_vec(dataset_train[index_non_zero, :], dataset_train[j, :]) for j in range(N)]))\n",
        "\n",
        "  B[i] = - in_labels[index_non_zero] + w_phi\n",
        "\n",
        "  print('%s done' % genres[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd-Vm9IiOpzK",
        "outputId": "069cd95d-8725-432f-b839-ec9351b91106"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blues done, start testing\n",
            "classical done, start testing\n",
            "country done, start testing\n",
            "disco done, start testing\n",
            "pop done, start testing\n",
            "rock done, start testing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier_oneToRest(input):\n",
        "  classifications = -1000 * np.ones(len(genres))\n",
        "\n",
        "  for genre in genres:\n",
        "    genre_index = genres.index(genre)\n",
        "\n",
        "    in_labels = np.zeros(N)\n",
        "    in_labels[partial_training_labels==genre_index]=1\n",
        "    in_labels[partial_training_labels!=genre_index]=-1\n",
        "\n",
        "    w_phi = np.sum(np.array([C[genre_index, i]*in_labels[i]*kernel_vec(input, partial_training_set[i, :]) for i in range(N)]))\n",
        "\n",
        "    classifications[genre_index] = w_phi - B[genre_index]\n",
        "\n",
        "  return genres[np.argmax(classifications)]"
      ],
      "metadata": {
        "id": "YU7elIVjS0MN"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix = np.zeros((len(genre_names), len(genre_names)))\n",
        "\n",
        "# Filter to only select samples from the input genres\n",
        "valid_set_filter = np.zeros(dataset_valid.shape[0], dtype='bool')\n",
        "for j in range(dataset_valid.shape[0]):\n",
        "  valid_set_filter[j] = labels_valid[j] in genre_indices\n",
        "\n",
        "# Application of filter on the test set\n",
        "partial_valid_set = dataset_valid[valid_set_filter,:]\n",
        "partial_valid_labels = labels_valid[valid_set_filter]\n",
        "\n",
        "for i in range(partial_valid_set.shape[0]):\n",
        "  predicted = genre_names.index(classifier_oneToRest(partial_valid_set[i, :]))\n",
        "  confusion_matrix [partial_valid_labels[i].astype(int), predicted] += 1\n",
        "\n",
        "print(confusion_matrix)\n",
        "print('Accuracy %1.2f %%' % (confusion_matrix.trace()/confusion_matrix.sum() * 100.0))"
      ],
      "metadata": {
        "id": "hy2FwbaZzt4Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9285acd9-70e5-4db2-a875-227fa7340d99"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1.  3.  0.  0.  0.  0.  0. 16.  0.  0.]\n",
            " [ 0. 17.  0.  2.  0.  0.  0.  6.  0.  0.]\n",
            " [ 0.  4.  1.  1.  0.  0.  0. 17.  0.  0.]\n",
            " [ 0.  3.  0.  2.  0.  0.  0. 12.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0. 19.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  4.  0.  2.  0.  0.  0. 11.  0.  0.]]\n",
            "Accuracy 33.06 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 genres"
      ],
      "metadata": {
        "id": "wyzCqCmPTkbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "genres = ['classical', 'pop']\n",
        "\n",
        "# Indices of genres to classify\n",
        "genre_indices = list()\n",
        "for genre_n in genres: genre_indices.append(genre_names.index(genre_n))\n",
        "\n",
        "# Filter to only select samples from the input genres\n",
        "training_set_filter = np.zeros(dataset_train.shape[0], dtype='bool')\n",
        "for j in range(dataset_train.shape[0]):\n",
        "  training_set_filter[j] = labels_train[j] in genre_indices\n",
        "\n",
        "# Application of filter on the test set\n",
        "partial_training_set = dataset_train[training_set_filter,:]\n",
        "partial_training_labels = labels_train[training_set_filter]\n",
        "\n",
        "N= partial_training_set.shape[0]"
      ],
      "metadata": {
        "id": "RzIxHnJ1TvXc"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The results of the dual problem computed\n",
        "C= np.zeros((len(genres), N))\n",
        "\n",
        "B = np.zeros(len(genres))\n",
        "\n",
        "#Iteration on the genres\n",
        "for i in range(len(genres)):\n",
        "  #Data of the genre considered\n",
        "  class_genre = partial_training_set[partial_training_labels==i, :]\n",
        "\n",
        "  #Data of the other genres\n",
        "  class_others = partial_training_set[partial_training_labels!=i, :]\n",
        "\n",
        "  #Data and labels used in the resolution of the dual problem\n",
        "  in_data = np.concatenate((class_genre, class_others), axis=0)\n",
        "  in_labels = np.concatenate((np.ones(class_genre.shape[0]), -1 * np.ones(class_others.shape[0])), axis=None)\n",
        "\n",
        "  a = np.zeros(N)\n",
        "  obj_k_jit=jax.jit(obj_kernel)\n",
        "\n",
        "  linear_constraint = opt.LinearConstraint(in_labels, 0, 0, keep_feasible=True)\n",
        "  res = opt.minimize(obj_k_jit, a, method='trust-constr', jac='2-point', hess=hessian, constraints=[linear_constraint], options={'maxiter': 1000}, bounds=opt.Bounds(np.zeros(N), np.ones(N)*np.inf))\n",
        "\n",
        "  C[i, :] = np.array(res.x)\n",
        "\n",
        "  index_non_zero = -1\n",
        "  for j in range(N):\n",
        "    if C[i, j] > 0 and index_non_zero < 0:\n",
        "      index_non_zero = j\n",
        "      break\n",
        "\n",
        "  w_phi = np.sum(np.array([C[i, j]*in_labels[j]*kernel_vec(dataset_train[index_non_zero, :], dataset_train[j, :]) for j in range(N)]))\n",
        "\n",
        "  B[i] = - in_labels[index_non_zero] + w_phi\n",
        "\n",
        "  print('%s done' % genres[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1546vGTUClM",
        "outputId": "ce8b7624-882e-4317-fbcd-8025c5705c09"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "classical done\n",
            "pop done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def classifier_oneToRest(input):\n",
        "  classifications = -1000 * np.ones(len(genres))\n",
        "\n",
        "  for genre in genres:\n",
        "    genre_index = genres.index(genre)\n",
        "\n",
        "    in_labels = np.zeros(N)\n",
        "    in_labels[partial_training_labels==genre_index]=1\n",
        "    in_labels[partial_training_labels!=genre_index]=-1\n",
        "\n",
        "    w_phi = np.sum(np.array([C[genre_index, i]*in_labels[i]*kernel_vec(input, partial_training_set[i, :]) for i in range(N)]))\n",
        "\n",
        "    classifications[genre_index] = w_phi - B[genre_index]\n",
        "\n",
        "  return genres[np.argmax(classifications)]"
      ],
      "metadata": {
        "id": "H8vAHnyWUFJg"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confusion_matrix = np.zeros((len(genre_names), len(genre_names)))\n",
        "\n",
        "# Filter to only select samples from the input genres\n",
        "valid_set_filter = np.zeros(dataset_valid.shape[0], dtype='bool')\n",
        "for j in range(dataset_valid.shape[0]):\n",
        "  valid_set_filter[j] = labels_valid[j] in genre_indices\n",
        "\n",
        "# Application of filter on the test set\n",
        "partial_valid_set = dataset_valid[valid_set_filter,:]\n",
        "partial_valid_labels = labels_valid[valid_set_filter]\n",
        "\n",
        "for i in range(partial_valid_set.shape[0]):\n",
        "  predicted = genre_names.index(classifier_oneToRest(partial_valid_set[i, :]))\n",
        "  confusion_matrix [partial_valid_labels[i].astype(int), predicted] += 1\n",
        "\n",
        "print(confusion_matrix)\n",
        "print('Accuracy %1.2f %%' % (confusion_matrix.trace()/confusion_matrix.sum() * 100.0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04mJ9RoZUHMn",
        "outputId": "776b7b04-6e4b-454b-ec9c-b9ed82a47614"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0. 25.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0. 19.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
            "Accuracy 56.82 %\n"
          ]
        }
      ]
    }
  ]
}